<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Euan Ong</title>

  <meta name="author" content="Euan Ong">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4VJBBSSEN8"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-4VJBBSSEN8');
  </script>
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Euan Ong
                  </p>
                  <p>I am a final-year undergraduate at the University of Cambridge, studying Computer Science.
                  </p>
                  <p>
                    My ambition is to develop powerful, yet safe and <a
                      href="https://transformer-circuits.pub/">interpretable</a>
                    abstract reasoners, whose
                    internal
                    state and behaviour remain <a href="https://arxiv.org/abs/2305.02469">transparent to the end
                      user</a>.
                  </p>
                  <p>
                    To this end, I'm particularly interested in exploring how the mathematical toolkits we use to
                    understand and structure programs ‚Äí such as <a
                      href="https://ai.stanford.edu/blog/causal-abstraction/">formal methods</a>,
                    <a href="https://colah.github.io/posts/2015-09-NN-Types-FP/">types</a> and <a
                      href="https://cats.for.ai/">category theory</a> ‚Äí can
                    inspire new ways
                    to both reverse-engineer existing neural networks, and build scalable neurosymbolic systems.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:euan.l.y.ong+site@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="data/cv.pdf">CV</a> &nbsp;/&nbsp;
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?user=vT2qcI0AAAAJ&hl=en">Google Scholar</a>
                    &nbsp;/&nbsp;
                    <a href="https://twitter.com/euan_ong/">Twitter</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/euanong/">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://github.com/euanong/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/euanong.jpeg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/euanong_circle.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    So far, my research has broadly focused on studying the behaviour of neural networks <i>in
                      vitro</i>:
                    understanding both how they generalise when learning to perform abstract tasks, and
                    what
                    this tells us about the algorithms they've learned in order to do so.
                  </p>
                  <p>
                    I'm currently probing the foundations of <a href="https://arxiv.org/abs/2105.02761">neural
                      algorithmic
                      reasoning</a>, exploring attacks on vision-language models, and poking language model
                    representations
                    with a stick.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td colspan="2">
                  <h3 style="padding-left: 20px;">Published work</h3>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/lcm.png" alt="monoids" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://proceedings.mlr.press/v198/ong22a/ong22a.pdf">
                    <span class="papertitle">Learnable Commutative Monoids for Graph Neural Networks</span>
                  </a>
                  <br>
                  <strong>Euan Ong</strong>, <a href="https://petar-v.com/">Petar VelicÃåkovicÃÅ</a>
                  <br>
                  <em>Learning on Graphs</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2212.08541">arXiv</a>
                  /
                  <a href="https://openreview.net/forum?id=WtFobB28VDey">reviews</a>
                  /
                  <a href="https://learnable-commutative-monoids.notion.site/">project page</a>
                  <p>Using ideas from abstract algebra and functional programming, we built a new GNN aggregator that
                    beats the state of the art on complex aggregation problems (especially out-of-distribution) while
                    remaining efficient and parallelisable on large graphs.</p>
                </td>
              </tr>

              <tr>
                <td colspan="2">
                  <h3 style="padding-left: 20px;">Informal projects</h3>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/ddl.png" alt="attention heads" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <!-- <a href="https://example.com"> -->
                  <a>
                    <span class="papertitle">Dissecting Deep Learning for Systematic Generalisation</span>
                  </a>
                  <br>
                  <strong>Euan Ong</strong>, <a href="https://www.linkedin.com/in/etaash-katiyar-77108818b/">Etaash
                    Katiyar</a>, <a href="https://www.linkedin.com/in/kai-en-chong-a10a9a18b">Kai-En Chong</a>, <a
                    href="https://albertqjiang.github.io/">Albert
                    Qiaochu Jiang</a>
                  <br>
                  <em>Informal research</em>, 2021
                  <br>
                  <!-- <a href="https://example.com">project page</a>
                  /
                  <a href="https://example.com">arXiv</a>
                  /
                  <a href="https://example.com">reviews</a>
                  /
                  <a href="https://example.com">bibTeX</a> -->
                  <p>We investigated the capabilities of transformers to systematically generalise when learning to
                    recognise formal languages (such as
                    <span style="font-variant: small-caps;">Parity</span> and <span
                      style="font-variant: small-caps;">2-Dyck</span>), empirically corroborating various <a
                      href="https://arxiv.org/abs/1906.06755">theoretical claims</a> about transformer generalisation.
                    Inspired
                    by our observations, we
                    derived a parallel, stackless algorithm for recognising <span
                      style="font-variant: small-caps;">2-Dyck</span> that could (in principle) be
                    implemented by a transformer with a constant number of attention layers.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
                  <img src="images/oxtd.png" alt="oxtd" width="140" height="140">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://drive.google.com/file/d/1UKkUcnq0CX-aq3c3o_-9xJdy7rKsytZ6/view?usp=sharing">
                    <span class="papertitle">Object Detection in Thermal Imagery via Convolutional Neural
                      Networks</span>
                  </a>
                  <br>
                  <strong>Euan Ong</strong>, <a href="https://www.cs.ox.ac.uk/people/niki.trigoni/">Niki Trigoni</a>, <a
                    href="https://portobgusmao.com/">Pedro Porto Barque de Gusm√£o</a>
                  <br>
                  <em>Technical report</em>, 2019
                  <br>
                  <!-- <a href="https://example.com">project page</a>
                  /
                  <a href="https://example.com">arXiv</a>
                  /
                  <a href="https://example.com">reviews</a>
                  /
                  <a href="https://example.com">bibTeX</a> -->
                  <p>
                    We trained a Faster R-CNN object detection network to identify landmarks (e.g. doors and windows) in
                    thermal images of indoor environments, with applications in the development of navigational aids for
                    search and rescue operations.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- 
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Community</h2>
                </td>
              </tr>
            </tbody>
          </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Miscellanea</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
                <td width="75%" valign="center">
                  <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                  <br>
                  <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                  <br>
                  <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member,
                    CVPR 2021</a>
                  <br>
                  <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                  <br>
                  <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cs188.jpg" alt="cs188">
                </td>
                <td width="75%" valign="center">
                  <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor,
                    CS188 Spring 2011</a>
                  <br>
                  <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor,
                    CS188 Fall 2010</a>
                  <br>
                  <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd
                    Edition</a>
                </td>
              </tr>


              <tr>
                <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                  <h2>Basically <br> Blog Posts</h2>
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                  <br>
                  <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain
                    Functions</a>
                  <br>
                  <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                  <br>
                  <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How
                    Can Academics Adapt?</a>
                </td>
              </tr>


            </tbody>
          </table> -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Design inspired by <a href="https://github.com/jonbarron/jonbarron_website"
                      style="font-size:small">Jon Barron</a>'s site.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>